{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "operating-dealer",
   "metadata": {
    "id": "operating-dealer",
    "outputId": "379db1d4-cb96-41c8-9ec5-e68fcf714525",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 17:39:08.096018: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 17:39:10.322960: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-13 17:39:10.326938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-13 17:39:10.408034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-13 17:39:10.408453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 computeCapability: 7.5\n",
      "coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 3.81GiB deviceMemoryBandwidth: 119.24GiB/s\n",
      "2022-04-13 17:39:10.408491: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-13 17:39:10.435785: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-04-13 17:39:10.435910: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-04-13 17:39:10.452401: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-13 17:39:10.456552: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-13 17:39:10.488200: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-13 17:39:10.494270: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-04-13 17:39:10.554052: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-13 17:39:10.554420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-13 17:39:10.555100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-13 17:39:10.555476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smart-sector",
   "metadata": {
    "id": "smart-sector",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Lambda, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "from collections import deque\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "import math\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d61a39ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 17:39:11.362321: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-13 17:39:11.362967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-13 17:39:11.363177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 computeCapability: 7.5\n",
      "coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 3.81GiB deviceMemoryBandwidth: 119.24GiB/s\n",
      "2022-04-13 17:39:11.363210: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-13 17:39:11.363244: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-04-13 17:39:11.363263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-04-13 17:39:11.363282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-13 17:39:11.363301: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-13 17:39:11.363320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-13 17:39:11.363339: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-04-13 17:39:11.363358: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-04-13 17:39:11.363437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-13 17:39:11.363631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-13 17:39:11.363767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-13 17:39:11.364123: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-04-13 17:39:12.406682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-13 17:39:12.406723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-04-13 17:39:12.406733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-04-13 17:39:12.407215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-13 17:39:12.407457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-13 17:39:12.407655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-13 17:39:12.407814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3067 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "2022-04-13 17:39:12.409003: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3e6bebec860b4c33\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3e6bebec860b4c33\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# rm -rf ./logs/train_PPO/\n",
    "train_log_dir = \"logs/train_PPO/\" + current_time\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {train_log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af99d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "total_timesteps = 25_000\n",
    "init_lr = 2.5e-4\n",
    "seed = 1\n",
    "num_envs = 4\n",
    "num_steps = 128\n",
    "batch_size = num_envs * num_steps\n",
    "anneal_lr = True\n",
    "gamma = 0.99\n",
    "lamda = 0.95\n",
    "gae = True\n",
    "num_minibatch = 4\n",
    "minibatch_size = batch_size//num_minibatch\n",
    "update_epochs = 4\n",
    "norm_adv = True\n",
    "clip_coeff = 0.2\n",
    "clip_value_loss = True\n",
    "entropy_coeff = 0.01\n",
    "valueloss_coeff = 0.5\n",
    "max_grad_norm = 0.5\n",
    "target_kl = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af11867",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "tough-synthetic",
   "metadata": {
    "id": "tough-synthetic",
    "outputId": "ecde7b79-c3d4-448c-e6be-d4a7884750f0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/anaconda3/envs/myenv/lib/python3.9/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/himanshu/Documents/my_project5/RLandNEAT/PPO/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "env = gym.wrappers.RecordVideo(env, \"videos\", step_trigger=lambda t: t%100==0)\n",
    "env.seed(seed)\n",
    "\n",
    "observation = env.reset()\n",
    "input_size = len(observation)\n",
    "output_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470de8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f687d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations Shape: (4,)\n",
      "Action Shape: ()\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observations Shape: {env.observation_space.shape}\")\n",
    "print(f\"Action Shape: {env.action_space.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b50afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "        \n",
    "    def build_critic(self):\n",
    "        inputs = Input(shape = input_size)\n",
    "        x = Dense(64, activation = 'tanh')(inputs)\n",
    "        x = Dense(64, activation = 'tanh')(x)\n",
    "        x = Dense(1, activation = 'linear')(x)\n",
    "        model = Model(inputs = inputs, outputs = x)\n",
    "        return model\n",
    "    \n",
    "    def build_actor(self):\n",
    "        inputs = Input(shape = input_size)\n",
    "        x = Dense(64, activation = 'tanh')(inputs)\n",
    "        x = Dense(64, activation = 'tanh')(x)\n",
    "        x = Dense(output_size, activation = 'linear')(x)\n",
    "        model = Model(inputs = inputs, outputs = x)\n",
    "        return model\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_actor_critic_value(self, x, action = None):\n",
    "        logits = self.actor(x)\n",
    "        dis = tfp.distributions.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = dis.sample()\n",
    "        \n",
    "        return np.squeeze(action), dis.log_prob(action), dis.entropy(), tf.squeeze(self.critic(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a26be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa027fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 17:39:15.768720: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step=16, Episodic_return=16.0\n",
      "Global Step=31, Episodic_return=15.0\n",
      "Global Step=48, Episodic_return=17.0\n",
      "Global Step=62, Episodic_return=14.0\n",
      "Global Step=90, Episodic_return=28.0\n",
      "Global Step=101, Episodic_return=11.0\n",
      "Global Step=113, Episodic_return=12.0\n",
      "Global Step=139, Episodic_return=11.0\n",
      "Global Step=199, Episodic_return=60.0\n",
      "Global Step=217, Episodic_return=18.0\n",
      "Global Step=236, Episodic_return=19.0\n",
      "Global Step=250, Episodic_return=14.0\n",
      "Global Step=277, Episodic_return=21.0\n",
      "Global Step=314, Episodic_return=37.0\n",
      "Global Step=355, Episodic_return=41.0\n",
      "Global Step=373, Episodic_return=18.0\n",
      "Global Step=412, Episodic_return=28.0\n",
      "Global Step=433, Episodic_return=21.0\n",
      "Global Step=448, Episodic_return=15.0\n",
      "Global Step=474, Episodic_return=26.0\n",
      "Global Step=496, Episodic_return=22.0\n",
      "SPS: 121\n",
      "Global Step=552, Episodic_return=40.0\n",
      "Global Step=568, Episodic_return=16.0\n",
      "Global Step=594, Episodic_return=26.0\n",
      "Global Step=608, Episodic_return=14.0\n",
      "Global Step=671, Episodic_return=31.0\n",
      "Global Step=691, Episodic_return=20.0\n",
      "Global Step=705, Episodic_return=14.0\n",
      "Global Step=725, Episodic_return=20.0\n",
      "Global Step=745, Episodic_return=20.0\n",
      "Global Step=756, Episodic_return=11.0\n",
      "Global Step=834, Episodic_return=66.0\n",
      "Global Step=859, Episodic_return=25.0\n",
      "Global Step=876, Episodic_return=17.0\n",
      "Global Step=892, Episodic_return=16.0\n",
      "Global Step=905, Episodic_return=9.0\n",
      "Global Step=948, Episodic_return=43.0\n",
      "Global Step=975, Episodic_return=27.0\n",
      "Global Step=1006, Episodic_return=31.0\n",
      "Global Step=1020, Episodic_return=14.0\n",
      "SPS: 130\n",
      "Global Step=1055, Episodic_return=31.0\n",
      "Global Step=1072, Episodic_return=17.0\n",
      "Global Step=1092, Episodic_return=20.0\n",
      "Global Step=1105, Episodic_return=13.0\n",
      "Global Step=1122, Episodic_return=17.0\n",
      "Global Step=1140, Episodic_return=18.0\n",
      "Global Step=1166, Episodic_return=14.0\n",
      "Global Step=1198, Episodic_return=32.0\n",
      "Global Step=1278, Episodic_return=80.0\n",
      "Global Step=1322, Episodic_return=42.0\n",
      "Global Step=1393, Episodic_return=71.0\n",
      "Global Step=1467, Episodic_return=59.0\n",
      "Global Step=1523, Episodic_return=56.0\n",
      "SPS: 134\n",
      "Global Step=1595, Episodic_return=59.0\n",
      "Global Step=1629, Episodic_return=34.0\n",
      "Global Step=1709, Episodic_return=45.0\n",
      "Global Step=1747, Episodic_return=38.0\n",
      "Global Step=1788, Episodic_return=41.0\n",
      "Global Step=1890, Episodic_return=98.0\n",
      "Global Step=1918, Episodic_return=28.0\n",
      "Global Step=1960, Episodic_return=40.0\n",
      "SPS: 137\n",
      "Global Step=2063, Episodic_return=15.0\n",
      "Global Step=2090, Episodic_return=27.0\n",
      "Global Step=2141, Episodic_return=51.0\n",
      "Global Step=2226, Episodic_return=50.0\n",
      "Global Step=2293, Episodic_return=67.0\n",
      "Global Step=2338, Episodic_return=34.0\n",
      "Global Step=2383, Episodic_return=45.0\n",
      "Global Step=2498, Episodic_return=66.0\n",
      "SPS: 139\n",
      "Global Step=2579, Episodic_return=19.0\n",
      "Global Step=2609, Episodic_return=30.0\n",
      "Global Step=2737, Episodic_return=49.0\n",
      "Global Step=2840, Episodic_return=24.0\n",
      "Global Step=2904, Episodic_return=64.0\n",
      "Global Step=2958, Episodic_return=14.0\n",
      "Global Step=3005, Episodic_return=47.0\n",
      "Global Step=3028, Episodic_return=23.0\n",
      "SPS: 140\n",
      "Global Step=3118, Episodic_return=46.0\n",
      "Global Step=3180, Episodic_return=62.0\n",
      "Global Step=3251, Episodic_return=51.0\n",
      "Global Step=3307, Episodic_return=56.0\n",
      "Global Step=3367, Episodic_return=39.0\n",
      "Global Step=3399, Episodic_return=32.0\n",
      "Global Step=3504, Episodic_return=48.0\n",
      "Global Step=3536, Episodic_return=32.0\n",
      "SPS: 141\n",
      "Global Step=3762, Episodic_return=50.0\n",
      "Global Step=3856, Episodic_return=16.0\n",
      "Global Step=3928, Episodic_return=72.0\n",
      "Global Step=4068, Episodic_return=100.0\n",
      "SPS: 142\n",
      "Global Step=4283, Episodic_return=59.0\n",
      "Global Step=4476, Episodic_return=124.0\n",
      "Global Step=4606, Episodic_return=126.0\n",
      "SPS: 142\n",
      "Global Step=4903, Episodic_return=39.0\n",
      "SPS: 143\n",
      "Global Step=5226, Episodic_return=106.0\n",
      "Global Step=5301, Episodic_return=53.0\n",
      "Global Step=5403, Episodic_return=27.0\n",
      "Global Step=5612, Episodic_return=108.0\n",
      "SPS: 143\n",
      "Global Step=5694, Episodic_return=62.0\n",
      "Global Step=5802, Episodic_return=42.0\n",
      "SPS: 143\n",
      "Global Step=6588, Episodic_return=60.0\n",
      "SPS: 143\n",
      "Global Step=6879, Episodic_return=95.0\n",
      "Global Step=6955, Episodic_return=43.0\n",
      "Global Step=7128, Episodic_return=88.0\n",
      "SPS: 143\n",
      "Global Step=7483, Episodic_return=59.0\n",
      "Global Step=7666, Episodic_return=114.0\n",
      "SPS: 143\n",
      "SPS: 143\n",
      "Global Step=8439, Episodic_return=119.0\n",
      "Global Step=8554, Episodic_return=106.0\n",
      "SPS: 143\n",
      "SPS: 143\n",
      "Global Step=9419, Episodic_return=75.0\n",
      "SPS: 143\n",
      "Global Step=9855, Episodic_return=127.0\n",
      "SPS: 143\n",
      "Global Step=10623, Episodic_return=127.0\n",
      "Global Step=10745, Episodic_return=121.0\n",
      "SPS: 143\n",
      "Global Step=11032, Episodic_return=24.0\n",
      "SPS: 143\n",
      "Global Step=11374, Episodic_return=110.0\n",
      "SPS: 143\n",
      "Global Step=12288, Episodic_return=128.0\n",
      "SPS: 143\n",
      "Global Step=12415, Episodic_return=127.0\n",
      "SPS: 144\n",
      "Global Step=12881, Episodic_return=81.0\n",
      "Global Step=13164, Episodic_return=108.0\n",
      "SPS: 144\n",
      "Global Step=13546, Episodic_return=106.0\n",
      "SPS: 144\n",
      "Global Step=14322, Episodic_return=114.0\n",
      "SPS: 144\n",
      "SPS: 144\n",
      "SPS: 144\n",
      "Global Step=15612, Episodic_return=124.0\n",
      "SPS: 144\n",
      "Global Step=15979, Episodic_return=107.0\n",
      "SPS: 144\n",
      "Global Step=16615, Episodic_return=103.0\n",
      "Global Step=16880, Episodic_return=112.0\n",
      "SPS: 145\n",
      "Global Step=16975, Episodic_return=79.0\n",
      "SPS: 145\n",
      "Global Step=17528, Episodic_return=120.0\n",
      "Global Step=17758, Episodic_return=94.0\n",
      "Global Step=17907, Episodic_return=115.0\n",
      "SPS: 145\n",
      "Global Step=17984, Episodic_return=64.0\n",
      "Global Step=18162, Episodic_return=114.0\n",
      "Global Step=18363, Episodic_return=59.0\n",
      "SPS: 145\n",
      "SPS: 145\n",
      "Global Step=19194, Episodic_return=122.0\n",
      "Global Step=19294, Episodic_return=94.0\n",
      "SPS: 145\n",
      "SPS: 145\n",
      "SPS: 145\n",
      "SPS: 145\n",
      "Global Step=21092, Episodic_return=100.0\n",
      "SPS: 145\n",
      "Global Step=21981, Episodic_return=93.0\n",
      "SPS: 145\n",
      "SPS: 145\n",
      "Global Step=22908, Episodic_return=124.0\n",
      "Global Step=23039, Episodic_return=127.0\n",
      "SPS: 145\n",
      "Global Step=23405, Episodic_return=109.0\n",
      "SPS: 145\n",
      "SPS: 145\n",
      "Global Step=24090, Episodic_return=26.0\n",
      "Global Step=24446, Episodic_return=126.0\n",
      "SPS: 145\n"
     ]
    }
   ],
   "source": [
    "states = np.zeros(shape = (num_steps, num_envs) + env.observation_space.shape, dtype = np.float32)\n",
    "actions = np.zeros(shape = ((num_steps, num_envs)+ env.action_space.shape), dtype = np.float32)\n",
    "log_probs = np.zeros(shape = (num_steps, num_envs), dtype = np.float32)\n",
    "rewards = np.zeros(shape = (num_steps, num_envs), dtype = np.float32)\n",
    "dones = np.zeros(shape = (num_steps, num_envs), dtype = np.float32)\n",
    "values = np.zeros(shape = (num_steps, num_envs), dtype = np.float32)\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_states = np.array([env.reset() for _ in range(num_envs)])\n",
    "next_dones = np.zeros(num_envs)\n",
    "num_updates = total_timesteps//batch_size\n",
    "optimizer = Adam(epsilon = 1e-5)\n",
    "\n",
    "for update in range(1, num_updates + 1):\n",
    "    if anneal_lr:\n",
    "        frac = 1.0 - (update - 1.0)/num_updates\n",
    "        lr_now = frac * init_lr\n",
    "        optimizer.lr = lr_now\n",
    "    \n",
    "    for e in range(num_envs):\n",
    "\n",
    "        \n",
    "        next_state = np.array(env.reset())\n",
    "        done = False\n",
    "        for step in range(num_steps):\n",
    "            global_step += 1 \n",
    "            states[step, e] = next_states[e]\n",
    "            dones[step, e] =  next_dones[e]\n",
    "            action, log_prob, _, value = agent.get_actor_critic_value(np.expand_dims(next_state, axis=0))\n",
    "            actions[step, e] = action\n",
    "            log_probs[step, e] = log_prob\n",
    "            values[step, e] = value\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            rewards[step, e] = reward\n",
    "\n",
    "            next_states[e] = next_state\n",
    "            next_dones[e] = done\n",
    "            \n",
    "            if \"episode\" in info.keys():\n",
    "                episodic_return = info[\"episode\"][\"r\"]\n",
    "                print(f\"Global Step={global_step}, Episodic_return={episodic_return}\")\n",
    "                with train_summary_writer.as_default(step = global_step):\n",
    "                    tf.summary.scalar(\"Charts/episodic_return\", episodic_return, global_step)\n",
    "                    tf.summary.scalar(\"Charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            \n",
    "            \n",
    "        \n",
    "            if done and step < num_steps-1:\n",
    "                next_state = np.array(env.reset())\n",
    "                done = False\n",
    "        \n",
    "        \n",
    "        \n",
    "    next_value = np.squeeze(agent.get_value(next_states))\n",
    "    if gae:\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        last_gae = 0.0\n",
    "        for step in reversed(range(num_steps)):\n",
    "            if step == num_steps - 1:\n",
    "                next_non_terminal = 1.0 - next_dones\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - dones[step+1]\n",
    "                next_values = values[step+1]\n",
    "                \n",
    "            delta = rewards[step] + gamma * next_values * next_non_terminal - values[step]     \n",
    "            advantages[step] = last_gae = delta + gamma * lamda * next_non_terminal * last_gae\n",
    "        returns = advantages + values\n",
    "    else:\n",
    "        returns = np.zeros_like(rewards)\n",
    "        for step in reversed(range(num_steps)):\n",
    "            if step == num_steps -1:\n",
    "                next_non_terminal  = 1.0 - next_dones\n",
    "                next_return = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - dones[step+1]\n",
    "                next_return = returns[step+1]\n",
    "            returns[step] = rewards[step] + gamma * next_non_terminal * next_return\n",
    "        advantages = returns - values\n",
    "        \n",
    "    b_states = states.reshape((-1,)+ env.observation_space.shape)\n",
    "    b_actions = actions.reshape((-1,)+ env.action_space.shape)\n",
    "    b_log_probs = log_probs.reshape((-1,))\n",
    "    b_values = values.reshape((-1,))\n",
    "    b_advantages = advantages.reshape((-1,))\n",
    "    b_returns = returns.reshape((-1,))\n",
    "    \n",
    "    b_indx = np.arange(batch_size)\n",
    "    clip_fracs = []\n",
    "    for epoch in range(update_epochs):\n",
    "        np.random.shuffle(b_indx)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_indx = b_indx[start:end]\n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                \n",
    "                _, new_log_prob, new_entropy, new_value = agent.get_actor_critic_value(b_states[mb_indx], b_actions[mb_indx])\n",
    "                \n",
    "                log_ratio = new_log_prob - b_log_probs[mb_indx]\n",
    "                ratio = tf.math.exp(log_ratio)\n",
    "                \n",
    "                # debugging metrics\n",
    "                approx_kl = tf.reduce_mean((ratio - 1) - log_ratio)\n",
    "                clip_fracs += [tf.reduce_mean(tf.cast(tf.abs(ratio - 1.0) > clip_coeff, dtype = tf.float32))]\n",
    "                \n",
    "                clipped_ratio = tf.clip_by_value(ratio, 1-clip_coeff, 1+clip_coeff)\n",
    "                \n",
    "                mb_advantages = b_advantages[mb_indx]\n",
    "\n",
    "                if norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean())/(mb_advantages.std() + 1e-8)\n",
    "\n",
    "                ppo_loss = tf.reduce_mean(tf.math.maximum(-ratio * mb_advantages, -clipped_ratio * mb_advantages))\n",
    "\n",
    "                if clip_value_loss:\n",
    "                    v_loss_unclipped = (new_value - b_returns[mb_indx]) ** 2\n",
    "                    v_clipped = b_values[mb_indx] + tf.clip_by_value(new_value - b_values[mb_indx], -clip_coeff, clip_coeff)\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_indx]) ** 2\n",
    "                    v_loss = 0.5 * tf.reduce_mean(tf.math.maximum(v_loss_unclipped, v_loss_clipped))\n",
    "                else:\n",
    "                    v_loss = 0.5 * tf.reduce_mean((new_value - b_returns[mb_indx]) ** 2)\n",
    "                \n",
    "\n",
    "                entropy_loss = tf.reduce_mean(new_entropy)\n",
    "                loss = ppo_loss + valueloss_coeff * v_loss - entropy_coeff * entropy_loss\n",
    "            \n",
    "            actor_grads = tape.gradient(loss, agent.actor.trainable_weights)\n",
    "            critic_grads = tape.gradient(loss, agent.critic.trainable_weights)\n",
    "            \n",
    "            actor_grads = [tf.clip_by_norm(g, max_grad_norm) for g in actor_grads]\n",
    "            critic_grads = [tf.clip_by_norm(g, max_grad_norm) for g in critic_grads]\n",
    "            \n",
    "            optimizer.apply_gradients(zip(actor_grads, agent.actor.trainable_weights))\n",
    "            optimizer.apply_gradients(zip(critic_grads, agent.critic.trainable_weights))\n",
    "    \n",
    "        if target_kl is not None:\n",
    "            if approx_kl > target_kl:\n",
    "                break\n",
    "        \n",
    "    var_true = np.var(b_returns)\n",
    "    explained_var = np.nan if var_true == 0 else 1 - np.var(b_returns - b_values)/var_true\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"Charts/learning_rate\", optimizer.lr, global_step)\n",
    "        tf.summary.scalar(\"losses/value_loss\", v_loss, global_step)\n",
    "        tf.summary.scalar(\"losses/policy_loss\", ppo_loss, global_step)\n",
    "        tf.summary.scalar(\"losses/entropy\", entropy_loss, global_step)\n",
    "        tf.summary.scalar(\"losses/approx_kl\", approx_kl, global_step)\n",
    "        tf.summary.scalar(\"losses/clipfrac\", np.mean(clip_fracs), global_step)\n",
    "        tf.summary.scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "        tf.summary.scalar(\"Charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "env.close()\n",
    "# train_summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84fa460b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, -1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape + (-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a42542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodic_return=402.0, Episode Length: 402\n"
     ]
    }
   ],
   "source": [
    "next_state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    action, _, _, _ = agent.get_actor_critic_value(np.expand_dims(next_state, axis=0))\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    if \"episode\" in info.keys():\n",
    "        episodic_return = info[\"episode\"][\"r\"]\n",
    "        l = info[\"episode\"][\"l\"]\n",
    "        print(f\"Episodic_return={episodic_return}, Episode Length: {l}\")\n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08c00b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1677222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
